{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import config\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---number of train sample---\n",
      "500\n",
      "---number of test sample---\n",
      "100\n",
      "---number of features---\n",
      "8027\n"
     ]
    }
   ],
   "source": [
    "train_filename = os.path.join(config.base_path, \"data\",\"训练.xlsx\")\n",
    "test_filename = os.path.join(config.base_path, \"data\",\"测试A.xlsx\")\n",
    "train_raw = pd.read_excel(train_filename) #训练数据\n",
    "test_raw = pd.read_excel(test_filename) #测试数据\n",
    "train_data = train_raw.iloc[:,1:] \n",
    "train_data.index = train_raw.iloc[:,0].values\n",
    "test_data = test_raw.iloc[:,1:]\n",
    "test_data.index = test_raw.iloc[:,0].values\n",
    "\n",
    "print(\"---number of train sample---\")\n",
    "print(train_data.shape[0])\n",
    "print(\"---number of test sample---\")\n",
    "print(test_data.shape[0])\n",
    "print(\"---number of features---\")\n",
    "print(test_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这里人工筛了一些特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['312X1', '312X2', '312X3', '312X4', '312X5', '312X6', '312X7', '312X8', '312X9', '312X10', '312X11', '312X12', '312X13', '312X14', '312X15', '312X16', '312X17', '312X18', '312X19', '312X20', '312X21', '312X22', '312X23', '312X24', '312X25', '312X26', '312X27', '312X28', '312X29', '312X30', '312X31', '312X32', '312X33', '312X34', '312X35', '312X36', '312X37', '312X38', '312X39', '312X40', '312X41', '312X42', '312X43', '312X44', '312X45', '312X46', '312X47', '312X48', '312X49', '312X50', '340X1', '340X2', '340X3', '340X4', '340X5', '340X6', '340X7', '340X8', '340X9', '340X10']\n"
     ]
    }
   ],
   "source": [
    "drop_list = [\"312X\" + str(i) for i in range(1, 51)] #这几个列和TOOL(#1)这一列冗余了\n",
    "drop_list += [\"340X\" + str(i) for i in range(1,11)] #这几个列和tool这一列冗余了\n",
    "print(drop_list)\n",
    "train_data = train_data.drop(columns = drop_list)\n",
    "test_data = test_data.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOOL_ID', 'Tool', 'TOOL_ID (#1)', 'TOOL_ID (#2)', 'TOOL_ID (#3)', 'Tool (#1)', 'Tool (#2)', 'tool', 'tool (#1)', 'TOOL', 'TOOL (#1)', 'Tool (#3)', 'TOOL (#2)']\n"
     ]
    }
   ],
   "source": [
    "cols = list(test_data.columns)\n",
    "cols_tool = list(filter(lambda x : \"tool\" in x.lower(), cols))\n",
    "print(cols_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fillna with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--TOOL_ID: K done--\n",
      "--TOOL_ID: O done--\n",
      "--TOOL_ID: M done--\n",
      "--TOOL_ID: J done--\n",
      "--TOOL_ID: N done--\n",
      "--TOOL_ID: L done--\n",
      "--Tool: B done--\n",
      "--Tool: A done--\n",
      "--TOOL_ID (#1): E done--\n",
      "--TOOL_ID (#1): N done--\n",
      "--TOOL_ID (#2): E done--\n",
      "--TOOL_ID (#2): D done--\n",
      "--TOOL_ID (#2): C done--\n",
      "--TOOL_ID (#3): N0 done--\n",
      "--TOOL_ID (#3): E0 done--\n",
      "--Tool (#1): 2823 done--\n",
      "--Tool (#1): 329 done--\n",
      "--Tool (#1): 530 done--\n",
      "--Tool (#1): 1110 done--\n",
      "--Tool (#1): 215 done--\n",
      "--Tool (#1): 1113 done--\n",
      "--Tool (#1): 1018 done--\n",
      "--Tool (#1): 1245 done--\n",
      "--Tool (#2): C done--\n",
      "--Tool (#2): B done--\n",
      "--Tool (#2): A done--\n",
      "--tool: 2409 done--\n",
      "--tool: 3009 done--\n",
      "--tool: 4106 done--\n",
      "--tool: 4147 done--\n",
      "--tool (#1): U done--\n",
      "--tool (#1): V done--\n",
      "--tool (#1): Q done--\n",
      "--tool (#1): X done--\n",
      "--tool (#1): R done--\n",
      "--tool (#1): P done--\n",
      "--tool (#1): W done--\n",
      "--tool (#1): S done--\n",
      "--tool (#1): T done--\n",
      "--TOOL: D done--\n",
      "--TOOL: C done--\n",
      "--TOOL: B done--\n",
      "--TOOL (#1): YX1 done--\n",
      "--TOOL (#1): XY1 done--\n",
      "--Tool (#3): 6 done--\n",
      "--Tool (#3): 7 done--\n",
      "--Tool (#3): 8 done--\n",
      "--Tool (#3): 9 done--\n",
      "--Tool (#3): 10 done--\n",
      "--Tool (#3): 11 done--\n",
      "--Tool (#3): 12 done--\n",
      "--Tool (#3): 13 done--\n",
      "--Tool (#3): 14 done--\n",
      "--Tool (#3): 15 done--\n",
      "--Tool (#3): B done--\n",
      "--Tool (#3): A done--\n",
      "0      47.2\n",
      "1      47.2\n",
      "2      47.2\n",
      "3      47.4\n",
      "4      47.3\n",
      "5      47.3\n",
      "6      47.3\n",
      "7      47.4\n",
      "8      47.2\n",
      "9      47.3\n",
      "10     47.4\n",
      "11     47.4\n",
      "12     47.3\n",
      "13     47.4\n",
      "14     47.4\n",
      "15     47.4\n",
      "16     47.4\n",
      "17     47.5\n",
      "18     47.4\n",
      "19     47.2\n",
      "20     47.3\n",
      "21     47.3\n",
      "22     47.4\n",
      "23     47.3\n",
      "24     47.2\n",
      "25     47.2\n",
      "26     47.2\n",
      "27     47.4\n",
      "28     47.4\n",
      "29     47.4\n",
      "       ... \n",
      "470    47.9\n",
      "471    47.9\n",
      "472      48\n",
      "473      48\n",
      "474    47.9\n",
      "475    47.9\n",
      "476      48\n",
      "477      48\n",
      "478    47.9\n",
      "479    47.9\n",
      "480      48\n",
      "481    47.9\n",
      "482    47.9\n",
      "483    47.9\n",
      "484    47.9\n",
      "485    47.9\n",
      "486    47.9\n",
      "487    47.9\n",
      "488    47.9\n",
      "489    47.9\n",
      "490    47.9\n",
      "491    47.9\n",
      "492    47.9\n",
      "493    47.9\n",
      "494    47.9\n",
      "495    47.9\n",
      "496    47.9\n",
      "497    47.9\n",
      "498    47.9\n",
      "499    47.9\n",
      "Name: 312X217, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# mean_col = train_data.mean()\n",
    "# train_data_fillna = train_data.fillna(mean_col)\n",
    "# mean_col = test_data.mean()\n",
    "# test_data_fillna = test_data.fillna(mean_col)\n",
    "# np_all = np.vstack((train_data.values[:,:-1], test_data.values))\n",
    "# df_all = pd.DataFrame(np_all, columns = test_data.columns)\n",
    "for i in range(len(cols_tool) - 1):\n",
    "    df_tmp = train_data.loc[:,cols_tool[i]:cols_tool[i+1]].iloc[:,:-1]\n",
    "    tools = df_tmp[cols_tool[i]]\n",
    "    tool_set = list(set(tools))\n",
    "    for k, tool in enumerate(tool_set):\n",
    "        df_tool = df_tmp.loc[df_tmp[cols_tool[i]]==tool]\n",
    "        mean_tool = df_tool.mean()\n",
    "        df_tool = df_tool.fillna(mean_tool)\n",
    "        print(\"--%s: %s done--\" %(cols_tool[i], tool))\n",
    "        if k == 0:\n",
    "            train_tmp = df_tool.values\n",
    "        else:\n",
    "            train_tmp = np.vstack((train_tmp, df_tool.values))\n",
    "    if i == 0:\n",
    "        tool_frame = train_tmp\n",
    "    else:\n",
    "        tool_frame = np.hstack((tool_frame, train_tmp))\n",
    "        \n",
    "df_tmp = train_data.loc[:,cols_tool[-1]:]\n",
    "tools = df_tmp[cols_tool[-1]]\n",
    "tool_set = list(set(tools))\n",
    "for k, tool in enumerate(tool_set):\n",
    "    df_tool = df_tmp.loc[df_tmp[cols_tool[-1]]==tool]\n",
    "    mean_tool = df_tool.mean()\n",
    "    df_tool = df_tool.fillna(mean_tool)\n",
    "    print(\"--%s: %s done--\" %(cols_tool[i], tool))\n",
    "    if k == 0:\n",
    "        train_tmp = df_tool.values\n",
    "    else:\n",
    "        train_tmp = np.vstack((train_tmp, df_tool.values))\n",
    "        \n",
    "tool_frame = np.hstack((tool_frame, train_tmp))\n",
    "train_data_fillna = pd.DataFrame(tool_frame, columns = train_data.columns)\n",
    "    \n",
    "# train_data_fillna = df_all[:train_data.shape[0]]\n",
    "# test_data_fillna = df_all[train_data.shape[0] + 1:]\n",
    "print(train_data_fillna[\"312X217\"])\n",
    "\n",
    "\n",
    "\n",
    "# np_all = np.vstack((train_data.values[:,:-1], test_data.values))\n",
    "# df_all = pd.DataFrame(np_all, columns = test_data.columns)\n",
    "# for i in range(len(cols_tool) - 1):\n",
    "#     df_tmp = df_all.loc[:,cols_tool[i]:cols_tool[i+1]].iloc[:,:-1]\n",
    "#     tools = df_tmp[cols_tool[i]]\n",
    "#     tool_set = list(set(tools))\n",
    "#     for k, tool in enumerate(tool_set):\n",
    "#         df_tool = df_tmp.loc[df_tmp[cols_tool[i]]==tool]\n",
    "#         mean_tool = df_tool.mean()\n",
    "#         df_tool.fillna(mean_tool, inplace = True)\n",
    "#         print(\"--%s: %s done--\" %(cols_tool[i], tool))\n",
    "\n",
    "        \n",
    "# df_tmp = df_all.loc[:,cols_tool[-1]:]\n",
    "# tools = df_tmp[cols_tool[-1]]\n",
    "# tool_set = set(tools)\n",
    "# for tool in tool_set:\n",
    "#     df_tool = df_tmp.loc[df_tmp[cols_tool[-1]]==tool]\n",
    "#     mean_tool = df_tool.mean()\n",
    "#     df_tool = df_tool.fillna(mean_tool, inplace = True)\n",
    "#     print(\"--%s: %s done--\" %(cols_tool[i], tool))\n",
    "    \n",
    "# train_data_fillna = df_all[:train_data.shape[0]]\n",
    "# test_data_fillna = df_all[train_data.shape[0] + 1:]\n",
    "# print(train_data_fillna[\"312X217\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2823\n",
      "1      2823\n",
      "2      2823\n",
      "3      2823\n",
      "4      2823\n",
      "5      2823\n",
      "6      2823\n",
      "7      2823\n",
      "8      2823\n",
      "9      2823\n",
      "10     2823\n",
      "11     2823\n",
      "12     2823\n",
      "13     2823\n",
      "14     2823\n",
      "15     2823\n",
      "16     2823\n",
      "17     2823\n",
      "18     2823\n",
      "19     2823\n",
      "20     2823\n",
      "21     2823\n",
      "22     2823\n",
      "23     2823\n",
      "24     2823\n",
      "25     2823\n",
      "26     2823\n",
      "27     2823\n",
      "28     2823\n",
      "29     2823\n",
      "       ... \n",
      "470    1245\n",
      "471    1245\n",
      "472    1245\n",
      "473    1245\n",
      "474    1245\n",
      "475    1245\n",
      "476    1245\n",
      "477    1245\n",
      "478    1245\n",
      "479    1245\n",
      "480    1245\n",
      "481    1245\n",
      "482    1245\n",
      "483    1245\n",
      "484    1245\n",
      "485    1245\n",
      "486    1245\n",
      "487    1245\n",
      "488    1245\n",
      "489    1245\n",
      "490    1245\n",
      "491    1245\n",
      "492    1245\n",
      "493    1245\n",
      "494    1245\n",
      "495    1245\n",
      "496    1245\n",
      "497    1245\n",
      "498    1245\n",
      "499    1245\n",
      "Name: Tool (#1), Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_data_fillna[\"Tool (#1)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onehot编码，删除一些重复的特征和时间戳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---number of features---\n",
      "6820\n"
     ]
    }
   ],
   "source": [
    "selected_features = [] #保留的特征\n",
    "dropped_features = [] #删除的特征\n",
    "for col in test_data_fillna.columns:\n",
    "    # 编码\n",
    "    x = np.hstack((train_data_fillna[col].values, test_data_fillna[col].values))\n",
    "    x_set_list = list(set(x))\n",
    "    # if col == '210X24':\n",
    "    #     print(x_set_list[0] / 10000000000)\n",
    "    #     exit()\n",
    "    if isinstance(x_set_list[0], str):\n",
    "        encode_dict = {}\n",
    "        for i in range(len(x_set_list)):\n",
    "            encode_dict[x_set_list[i]] = i\n",
    "        c_train = train_data_fillna[col]\n",
    "        for i in range(len(c_train)):\n",
    "            c_train.iat[i] = encode_dict[c_train.iat[i]]\n",
    "        c_test = test_data_fillna[col]\n",
    "        for i in range(len(c_test)):\n",
    "            c_test.iat[i] = encode_dict[c_test.iat[i]]\n",
    "    \n",
    "    #去重和去时间戳\n",
    "    x = train_data_fillna[col].values\n",
    "    x_set_list = list(set(x))\n",
    "\n",
    "    if len(x_set_list) >= 2 and not all([True if str(n) == \"nan\" else False for n in x]):\n",
    "        # if col == \"210X24\":\n",
    "        #     print(x_set_list[:20])\n",
    "        #     print(str(x_set_list[0]))\n",
    "        #     print([str(e).startswith(\"2017\") or str(e).startswith(\"2016\") for e in x_set_list[:20]])\n",
    "        if col == \"520X171\":\n",
    "            dropped_features.append(col)\n",
    "        elif not all([str(e).startswith(\"2017\") or str(e).startswith(\"2016\") for e in x_set_list[:20]]):\n",
    "            selected_features.append(col)\n",
    "        else:\n",
    "            dropped_features.append(col)\n",
    "    else:\n",
    "        dropped_features.append(col)\n",
    "df_train = train_data_fillna.loc[:, selected_features + ['Y']]\n",
    "df_test = test_data_fillna.loc[:, selected_features]\n",
    "\n",
    "print(\"---number of features---\")\n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 按照相关系数筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr_values = []\n",
    "k = 3000\n",
    "for col in df_test.columns:\n",
    "    corr_values.append(abs(pearsonr(df_train[col].values,df_train['Y'])[0]))\n",
    "corr_df = pd.DataFrame({'col':df_test.columns, 'corr_value':corr_values})\n",
    "corr_df = corr_df.sort_values(by='corr_value',ascending=False)\n",
    "selected = corr_df['col'].values[:k]\n",
    "\n",
    "df_train_corr = df_train.loc[:, list(selected) + ['Y']]\n",
    "df_test_corr = df_test.loc[:, list(selected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照树模型筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Xgboost feature importance: 238th---\n",
      "0.00363636\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "X_train = df_train.values[:, 0:-1]\n",
    "Y_train = df_train.values[:,-1]\n",
    "X_test = df_test.values[:,:]\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, random_state = 1024, test_size=0.1)\n",
    "\n",
    "# reg = GradientBoostingRegressor(random_state = 0,\n",
    "#                                 learning_rate = 0.1,\n",
    "#                                 n_estimators= 29,\n",
    "#                                 min_samples_split=2,\n",
    "#                                 min_samples_leaf=5,\n",
    "#                                 max_features=0.81,\n",
    "#                                 subsample= 0.8,\n",
    "#                                 max_depth= 6,\n",
    "# )\n",
    "reg = XGBRegressor(random_state = 0,\n",
    "                    learning_rate = 0.1,\n",
    "                    n_estimators= 49,\n",
    "                    subsample= 0.78,\n",
    "                    colsample_bytree= 0.62,\n",
    "                    max_depth= 3,\n",
    ")\n",
    "reg.fit(X_train, Y_train)\n",
    "sorted_imp = sorted(reg.feature_importances_, reverse=True)\n",
    "k = sorted_imp.index(0) - 1\n",
    "print(\"---Xgboost feature importance: %dth---\" %k)\n",
    "print(sorted_imp[k])\n",
    "importance_df = pd.DataFrame({'col':df_test.columns, 'importance':reg.feature_importances_})\n",
    "importance_df = importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "selected = importance_df['col'].values[:k + 1]\n",
    "df_train_tree = df_train.loc[:, list(selected) + ['Y']]\n",
    "df_test_tree = df_test.loc[:, list(selected)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# after_file = os.path.join(config.base_path, \"data\", \"tree_feature_selected_xgboost_A.xlsx\")\n",
    "# with pd.ExcelWriter(after_file) as writer:\n",
    "#     df_train_tree.to_excel(writer,sheet_name = \"train_data\")\n",
    "#     df_test_tree.to_excel(writer, sheet_name = \"test_data\")\n",
    "    \n",
    "after_file = os.path.join(config.base_path, \"data\", \"feature_selected_3000_A.xlsx\")\n",
    "with pd.ExcelWriter(after_file) as writer:\n",
    "    df_train.to_excel(writer,sheet_name = \"train_data\")\n",
    "    df_test.to_excel(writer, sheet_name = \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
