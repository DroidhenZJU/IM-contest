{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import config\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train sample : 499\n",
      "number of test sample : 100\n",
      "number of features : 8027\n",
      "number of all samples : 599\n"
     ]
    }
   ],
   "source": [
    "train_filename = os.path.join(config.base_path, \"data\",\"train.xlsx\")\n",
    "test_filename = os.path.join(config.base_path, \"data\",\"测试A.xlsx\")\n",
    "df_train_raw = pd.read_excel(train_filename) #训练数据\n",
    "df_test_raw = pd.read_excel(test_filename) #测试数据\n",
    "\n",
    "df_train = df_train_raw.iloc[:,1:] \n",
    "df_train.index = df_train_raw.iloc[:,0].values\n",
    "df_test = df_test_raw.iloc[:,1:]\n",
    "df_test.index = df_test_raw.iloc[:,0].values\n",
    "\n",
    "cols = df_train.columns\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "df_all = df_all[cols]\n",
    "print(\"number of train sample : %d\" %df_train.shape[0])\n",
    "print(\"number of test sample : %d\" %df_test.shape[0])\n",
    "print(\"number of features : %d\" %df_test.shape[1])\n",
    "print(\"number of all samples : %d\" %df_all.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除重复列和时间列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features after dropna: 3329\n",
      "number of features after drop_dumplicates: 3329\n",
      "['TOOL_ID', 'Tool', 'TOOL_ID (#1)', 'TOOL_ID (#2)', 'TOOL_ID (#3)', 'Tool (#1)', 'Tool (#2)', 'tool', 'tool (#1)', 'TOOL', 'TOOL (#1)', 'Tool (#3)', 'TOOL (#2)']\n",
      "ID001    100\n",
      "ID002    100\n",
      "ID003    100\n",
      "ID004    100\n",
      "ID005    100\n",
      "ID006    100\n",
      "ID007    100\n",
      "ID010    100\n",
      "ID011    100\n",
      "ID012    100\n",
      "ID013    100\n",
      "ID015    100\n",
      "ID018    100\n",
      "ID019    100\n",
      "ID032    100\n",
      "ID033    100\n",
      "ID034    100\n",
      "ID035    100\n",
      "ID036    100\n",
      "ID037    100\n",
      "ID040    100\n",
      "ID042    100\n",
      "ID044    100\n",
      "ID046    100\n",
      "ID049    100\n",
      "ID051    100\n",
      "ID056    100\n",
      "ID063    100\n",
      "ID064    100\n",
      "ID067    100\n",
      "        ... \n",
      "ID807    100\n",
      "ID811    100\n",
      "ID812    100\n",
      "ID813    100\n",
      "ID816    100\n",
      "ID817    100\n",
      "ID818    100\n",
      "ID819    100\n",
      "ID820    100\n",
      "ID821    100\n",
      "ID822    100\n",
      "ID823    NaN\n",
      "ID824    NaN\n",
      "ID825    100\n",
      "ID826    100\n",
      "ID833    100\n",
      "ID834    100\n",
      "ID835    100\n",
      "ID838    100\n",
      "ID839    100\n",
      "ID841    100\n",
      "ID844    100\n",
      "ID848    100\n",
      "ID849    100\n",
      "ID850    100\n",
      "ID851    100\n",
      "ID852    100\n",
      "ID853    100\n",
      "ID854    100\n",
      "ID855    100\n",
      "Name: 400X17, Length: 599, dtype: object\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [['520X171']] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ca1670b8e7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdropped_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdropped_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"520X171\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropped_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"重复的列和时间.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdf_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropped_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"number of features : %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[1;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[1;31m# ugly hack for GH #836\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[1;32m    206\u001b[0m                                  \u001b[1;34m\"[{types}] types\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1470\u001b[0m                         raise KeyError(\n\u001b[1;32m   1471\u001b[0m                             u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1472\u001b[0;31m                                 key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1473\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [['520X171']] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#去除掉全部为空的列\n",
    "df_all.dropna(axis=1, how='all', inplace=True)\n",
    "print(\"number of features after dropna: %d\" %df_all.shape[1])\n",
    "\n",
    "df_all = df_all.T.drop_duplicates().T\n",
    "print(\"number of features after drop_dumplicates: %d\" %df_all.shape[1])\n",
    "\n",
    "cols = df_all.columns\n",
    "cols_tool = list(filter(lambda x : \"tool\" in x.lower(), cols))\n",
    "print(cols_tool)\n",
    "\n",
    "# 去掉完全重复的列和时间列\n",
    "dropped_col = []\n",
    "for i in set(cols)-set(cols_tool):\n",
    "    if len(set(df_all[i])) <= 1:\n",
    "        dropped_col.append(i)\n",
    "    if all([str(e).startswith(\"2017\") or str(e).startswith(\"2016\") for e in df_all[i][:20]]):\n",
    "        dropped_col.append(i)\n",
    "dropped_col.append(\"520X171\")\n",
    "df_all.loc[:,dropped_col].to_csv(os.path.join(config.base_path, \"data\", \"重复的列和时间.csv\"))\n",
    "df_all = df_all.drop(labels=dropped_col, axis=1)\n",
    "print(\"number of features : %d\" %df_all.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID001    4\n",
      "ID002    3\n",
      "ID003    2\n",
      "ID004    3\n",
      "ID005    3\n",
      "ID006    3\n",
      "ID007    4\n",
      "ID010    4\n",
      "ID011    4\n",
      "ID012    3\n",
      "ID013    2\n",
      "ID015    0\n",
      "ID018    0\n",
      "ID019    0\n",
      "ID032    3\n",
      "ID033    2\n",
      "ID034    4\n",
      "ID035    3\n",
      "ID036    2\n",
      "ID037    4\n",
      "ID040    3\n",
      "ID042    2\n",
      "ID044    4\n",
      "ID046    3\n",
      "ID049    4\n",
      "ID051    2\n",
      "ID056    4\n",
      "ID063    2\n",
      "ID064    4\n",
      "ID067    4\n",
      "        ..\n",
      "ID807    5\n",
      "ID811    5\n",
      "ID812    5\n",
      "ID813    4\n",
      "ID816    3\n",
      "ID817    2\n",
      "ID818    4\n",
      "ID819    5\n",
      "ID820    5\n",
      "ID821    2\n",
      "ID822    3\n",
      "ID823    3\n",
      "ID824    2\n",
      "ID825    3\n",
      "ID826    3\n",
      "ID833    5\n",
      "ID834    5\n",
      "ID835    5\n",
      "ID838    2\n",
      "ID839    4\n",
      "ID841    4\n",
      "ID844    5\n",
      "ID848    5\n",
      "ID849    4\n",
      "ID850    2\n",
      "ID851    5\n",
      "ID852    5\n",
      "ID853    4\n",
      "ID854    1\n",
      "ID855    2\n",
      "Name: TOOL_ID, Length: 599, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "for col in cols_tool:\n",
    "    tools = df_all[col].values\n",
    "    df_all[col] = le.fit_transform(tools)\n",
    "print(df_all[cols_tool[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值填充和数据标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--TOOL_ID: 0 done--\n",
      "--TOOL_ID: 1 done--\n",
      "--TOOL_ID: 2 done--\n",
      "--TOOL_ID: 3 done--\n",
      "--TOOL_ID: 4 done--\n",
      "--TOOL_ID: 5 done--\n",
      "--Tool: 0 done--\n",
      "--Tool: 1 done--\n",
      "--TOOL_ID (#1): 0 done--\n",
      "--TOOL_ID (#1): 1 done--\n",
      "--TOOL_ID (#2): 0 done--\n",
      "--TOOL_ID (#2): 1 done--\n",
      "--TOOL_ID (#2): 2 done--\n",
      "--TOOL_ID (#3): 0 done--\n",
      "--TOOL_ID (#3): 1 done--\n",
      "--Tool (#1): 0 done--\n",
      "--Tool (#1): 1 done--\n",
      "--Tool (#1): 2 done--\n",
      "--Tool (#1): 3 done--\n",
      "--Tool (#1): 4 done--\n",
      "--Tool (#1): 5 done--\n",
      "--Tool (#1): 6 done--\n",
      "--Tool (#1): 7 done--\n",
      "--Tool (#1): 8 done--\n",
      "--Tool (#2): 0 done--\n",
      "--Tool (#2): 1 done--\n",
      "--Tool (#2): 2 done--\n",
      "--tool: 0 done--\n",
      "--tool: 1 done--\n",
      "--tool: 2 done--\n",
      "--tool: 3 done--\n",
      "--tool (#1): 0 done--\n",
      "--tool (#1): 1 done--\n",
      "--tool (#1): 2 done--\n",
      "--tool (#1): 3 done--\n",
      "--tool (#1): 4 done--\n",
      "--tool (#1): 5 done--\n",
      "--tool (#1): 6 done--\n",
      "--tool (#1): 7 done--\n",
      "--tool (#1): 8 done--\n",
      "--TOOL: 0 done--\n",
      "--TOOL: 1 done--\n",
      "--TOOL: 2 done--\n",
      "--TOOL (#1): 0 done--\n",
      "--TOOL (#1): 1 done--\n",
      "--Tool (#3): 0 done--\n",
      "--Tool (#3): 1 done--\n",
      "--Tool (#3): 2 done--\n",
      "--Tool (#3): 3 done--\n",
      "--Tool (#3): 4 done--\n",
      "--Tool (#3): 5 done--\n",
      "--Tool (#3): 6 done--\n",
      "--Tool (#3): 7 done--\n",
      "--Tool (#3): 8 done--\n",
      "--Tool (#3): 9 done--\n",
      "--Tool (#3): 10 done--\n",
      "--TOOL (#2): 0 done--\n",
      "--TOOL (#2): 1 done--\n"
     ]
    }
   ],
   "source": [
    "# 按照不同的TOOL类别填充均值\n",
    "cols = df_all.columns\n",
    "# cols_tool = list(filter(lambda x : \"tool\" in x.lower(), cols))\n",
    "# print(cols_tool)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "for i in range(len(cols_tool) - 1):\n",
    "    df_tmp = df_all.loc[:,cols_tool[i]:cols_tool[i+1]].iloc[:,:-1]\n",
    "    tools = df_tmp[cols_tool[i]]\n",
    "    tool_set = list(set(tools))\n",
    "    for k, tool in enumerate(tool_set):\n",
    "        df_tool = df_tmp.loc[df_tmp[cols_tool[i]]==tool]\n",
    "        mean_tool = df_tool.mean().fillna(0)\n",
    "        df_tool = df_tool.fillna(mean_tool)\n",
    "        \n",
    "        df_tool = pd.DataFrame(scaler.fit_transform(df_tool.values), index = df_tool.index, columns = df_tool.columns)\n",
    "\n",
    "        print(\"--%s: %s done--\" %(cols_tool[i], tool))\n",
    "        if k == 0:\n",
    "            tmp = df_tool\n",
    "        else:\n",
    "            tmp = pd.concat([tmp, df_tool])\n",
    "    if i == 0:\n",
    "        tool_frame = tmp\n",
    "    else:\n",
    "        tool_frame = pd.concat([tool_frame, tmp], axis = 1)\n",
    "        \n",
    "df_tmp = df_all.loc[:,cols_tool[-1]:].iloc[:,:-1]\n",
    "tools = df_tmp[cols_tool[-1]]\n",
    "tool_set = list(set(tools))\n",
    "for k, tool in enumerate(tool_set):\n",
    "    df_tool = df_tmp.loc[df_tmp[cols_tool[-1]]==tool]\n",
    "    mean_tool = df_tool.mean().fillna(0)\n",
    "    df_tool = df_tool.fillna(mean_tool)\n",
    "    df_tool = pd.DataFrame(scaler.fit_transform(df_tool.values), index = df_tool.index, columns = df_tool.columns)\n",
    "    print(\"--%s: %s done--\" %(cols_tool[-1], tool))\n",
    "    if k == 0:\n",
    "        tmp = df_tool\n",
    "    else:\n",
    "        tmp = pd.concat([tmp, df_tool])\n",
    "\n",
    "tool_frame = pd.concat([tool_frame, tmp, df_all[\"Y\"]], axis = 1)\n",
    "\n",
    "        \n",
    "df_all = tool_frame[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TOOL_ID', '210X1', '210X2', '210X3', '210X4', '210X5', '210X6',\n",
      "       '210X7', '210X8', '210X9',\n",
      "       ...\n",
      "       '750X1288', '750X1296', '750X1298', '750X1306', '750X1312', '750X1314',\n",
      "       '750X1330', '750X1357', '750X1384', 'Y'],\n",
      "      dtype='object', length=3329)\n",
      "Index(['ID716', 'ID717', 'ID719', 'ID720', 'ID721', 'ID722', 'ID723', 'ID724',\n",
      "       'ID725', 'ID727', 'ID728', 'ID729', 'ID730', 'ID731', 'ID732', 'ID733',\n",
      "       'ID734', 'ID735', 'ID736', 'ID737', 'ID738', 'ID739', 'ID740', 'ID741',\n",
      "       'ID742', 'ID743', 'ID744', 'ID745', 'ID746', 'ID747', 'ID749', 'ID751',\n",
      "       'ID752', 'ID753', 'ID757', 'ID759', 'ID760', 'ID761', 'ID762', 'ID763',\n",
      "       'ID764', 'ID765', 'ID766', 'ID768', 'ID769', 'ID770', 'ID771', 'ID772',\n",
      "       'ID773', 'ID774', 'ID776', 'ID777', 'ID778', 'ID779', 'ID781', 'ID783',\n",
      "       'ID785', 'ID786', 'ID787', 'ID788', 'ID790', 'ID792', 'ID793', 'ID797',\n",
      "       'ID798', 'ID801', 'ID802', 'ID803', 'ID804', 'ID806', 'ID807', 'ID811',\n",
      "       'ID812', 'ID813', 'ID816', 'ID817', 'ID818', 'ID819', 'ID820', 'ID821',\n",
      "       'ID822', 'ID823', 'ID824', 'ID825', 'ID826', 'ID833', 'ID834', 'ID835',\n",
      "       'ID838', 'ID839', 'ID841', 'ID844', 'ID848', 'ID849', 'ID850', 'ID851',\n",
      "       'ID852', 'ID853', 'ID854', 'ID855'],\n",
      "      dtype='object')\n",
      "number of train sample : 499\n",
      "number of test sample : 100\n",
      "number of features : 3328\n"
     ]
    }
   ],
   "source": [
    "#print(df_all_fillna.loc[\"ID563\",\"312X211\"])\n",
    "\n",
    "df_train = df_all.loc[df_train.index,:]\n",
    "df_test = df_all.loc[df_test.index,:].drop(\"Y\", axis = 1)\n",
    "print(df_train.columns)\n",
    "print(df_test.index)\n",
    "print(\"number of train sample : %d\" %df_train.shape[0])\n",
    "print(\"number of test sample : %d\" %df_test.shape[0])\n",
    "print(\"number of features : %d\" %df_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照相关系数筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda\\lib\\site-packages\\scipy\\stats\\stats.py:3029: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n",
      "D:\\Anaconda\\Anaconda\\lib\\site-packages\\scipy\\stats\\stats.py:5084: RuntimeWarning: invalid value encountered in less\n",
      "  x = np.where(x < 1.0, x, 1.0)  # if x > 1 then return 1.0\n"
     ]
    }
   ],
   "source": [
    "corr_values = []\n",
    "k = 3000\n",
    "for col in df_test.columns[:-1]:\n",
    "    corr_values.append(abs(pearsonr(df_train[col].values,df_train['Y'])[0]))\n",
    "corr_df = pd.DataFrame({'col':df_test.columns[:-1], 'corr_value':corr_values})\n",
    "corr_df = corr_df.sort_values(by='corr_value',ascending=False)\n",
    "selected = corr_df['col'].values[:k]\n",
    "\n",
    "df_train_corr = df_train.loc[:, list(selected) + ['Y']]\n",
    "df_test_corr = df_test.loc[:, list(selected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "after_file = os.path.join(config.base_path, \"data\", \"feature_selected_A_all_category_scale.xlsx\")\n",
    "with pd.ExcelWriter(after_file) as writer:\n",
    "    df_train.to_excel(writer,sheet_name = \"train_data\")\n",
    "    df_test.to_excel(writer, sheet_name = \"test_data\")\n",
    "\n",
    "after_file = os.path.join(config.base_path, \"data\", \"feature_selected_A_3000_category_scale.xlsx\")\n",
    "with pd.ExcelWriter(after_file) as writer:\n",
    "    df_train_corr.to_excel(writer,sheet_name = \"train_data\")\n",
    "    df_test_corr.to_excel(writer, sheet_name = \"test_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
